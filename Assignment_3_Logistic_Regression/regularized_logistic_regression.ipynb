{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "#### To classify News articles as belonging to two categories - business or sports (binary classification)\n",
    "\n",
    "Any editing needs to be done only in the cells marked with \"Tune hyperparameters here\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Useful notebook shortcuts:\n",
    "\n",
    "Ctrl+Enter -> Run current cell\n",
    "\n",
    "Shift+Enter -> Run current cell and go to next cell\n",
    "\n",
    "Alt+Enter -> Run current cell and add new cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "from model import *\n",
    "from feature import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Change the path to the training data directory\n",
    "sports = readfiles('train/sports')\n",
    "business = readfiles('train/business')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare bag-of-words based features (for the whole data)\n",
    "\n",
    "#### To run this block: \n",
    "\n",
    "Complete the `preprocess` and `extract` function in the `BagOfWordsFeatureExtractor` class in `feature.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize the model and preprocess\n",
    "bow = BagOfWordsFeatureExtractor()\n",
    "bow.preprocess(business + sports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract fetures and create a numpy array of features\n",
    "X_business_bow = bow.extract(business)\n",
    "X_sports_bow = bow.extract(sports)\n",
    "X_data_bow = np.concatenate((X_business_bow, X_sports_bow))\n",
    "\n",
    "# We know the correct labels, so create a numpy array for correct labels\n",
    "# Business -> 0, Sports -> 1\n",
    "Y_data_bow = np.concatenate((np.zeros(X_business_bow.shape[0]), np.ones(X_sports_bow.shape[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting into train and val sets\n",
    "\n",
    "General convention is to have a train/val split in the ratio of 70/30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_train = X_data_bow.shape[0]\n",
    "num_val = np.int(num_train*0.3)\n",
    "\n",
    "# Permute the indices to randomly split data into train and val\n",
    "data_idxs = np.random.permutation(num_train)\n",
    "\n",
    "# Separate train data\n",
    "X_train_bow = X_data_bow[data_idxs[num_val:], :]\n",
    "Y_train_bow = Y_data_bow[data_idxs[num_val:]]\n",
    "\n",
    "# Separate val data\n",
    "X_val_bow = X_data_bow[data_idxs[:num_val], :]\n",
    "Y_val_bow = Y_data_bow[data_idxs[:num_val]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune hyperparameters here\n",
    "The model will get trained and the loss at various iterations will be printed. Try and reduce this loss through hyperparameter tuning, to get a better validation set accuracy in the next block. However, don't chase the number to a 1.0, as the focus is on implementation rather than winning a contest.\n",
    "\n",
    "### To run this block: \n",
    "Complete the following functions in `model.py` of class `LogisticRegression`\n",
    "1. `loss`\n",
    "2. `train`\n",
    "3. `sigmoid`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 of 10000. Loss: 1.070766 LR: 0.000500\n",
      "Iteration 100 of 10000. Loss: 0.950779 LR: 0.000500\n",
      "Iteration 200 of 10000. Loss: 0.947475 LR: 0.000500\n",
      "Iteration 300 of 10000. Loss: 0.944221 LR: 0.000500\n",
      "Iteration 400 of 10000. Loss: 0.941018 LR: 0.000500\n",
      "Iteration 500 of 10000. Loss: 0.937864 LR: 0.000500\n",
      "Iteration 600 of 10000. Loss: 0.934759 LR: 0.000500\n",
      "Iteration 700 of 10000. Loss: 0.931703 LR: 0.000500\n",
      "Iteration 800 of 10000. Loss: 0.928694 LR: 0.000500\n",
      "Iteration 900 of 10000. Loss: 0.925733 LR: 0.000500\n",
      "Iteration 1000 of 10000. Loss: 0.922818 LR: 0.000500\n",
      "Iteration 1100 of 10000. Loss: 0.919950 LR: 0.000500\n",
      "Iteration 1200 of 10000. Loss: 0.917128 LR: 0.000500\n",
      "Iteration 1300 of 10000. Loss: 0.914351 LR: 0.000500\n",
      "Iteration 1400 of 10000. Loss: 0.911619 LR: 0.000500\n",
      "Iteration 1500 of 10000. Loss: 0.908931 LR: 0.000500\n",
      "Iteration 1600 of 10000. Loss: 0.906288 LR: 0.000500\n",
      "Iteration 1700 of 10000. Loss: 0.903687 LR: 0.000500\n",
      "Iteration 1800 of 10000. Loss: 0.901130 LR: 0.000500\n",
      "Iteration 1900 of 10000. Loss: 0.898615 LR: 0.000500\n",
      "Iteration 2000 of 10000. Loss: 0.896143 LR: 0.000500\n",
      "Iteration 2100 of 10000. Loss: 0.893711 LR: 0.000500\n",
      "Iteration 2200 of 10000. Loss: 0.891321 LR: 0.000500\n",
      "Iteration 2300 of 10000. Loss: 0.888972 LR: 0.000500\n",
      "Iteration 2400 of 10000. Loss: 0.886663 LR: 0.000500\n",
      "Iteration 2500 of 10000. Loss: 0.884394 LR: 0.000500\n",
      "Iteration 2600 of 10000. Loss: 0.882163 LR: 0.000500\n",
      "Iteration 2700 of 10000. Loss: 0.879972 LR: 0.000500\n",
      "Iteration 2800 of 10000. Loss: 0.877820 LR: 0.000500\n",
      "Iteration 2900 of 10000. Loss: 0.875705 LR: 0.000500\n",
      "Iteration 3000 of 10000. Loss: 0.873628 LR: 0.000500\n",
      "Iteration 3100 of 10000. Loss: 0.871588 LR: 0.000500\n",
      "Iteration 3200 of 10000. Loss: 0.869585 LR: 0.000500\n",
      "Iteration 3300 of 10000. Loss: 0.867619 LR: 0.000500\n",
      "Iteration 3400 of 10000. Loss: 0.865688 LR: 0.000500\n",
      "Iteration 3500 of 10000. Loss: 0.863793 LR: 0.000500\n",
      "Iteration 3600 of 10000. Loss: 0.861933 LR: 0.000500\n",
      "Iteration 3700 of 10000. Loss: 0.860108 LR: 0.000500\n",
      "Iteration 3800 of 10000. Loss: 0.858317 LR: 0.000500\n",
      "Iteration 3900 of 10000. Loss: 0.856561 LR: 0.000500\n",
      "Iteration 4000 of 10000. Loss: 0.854837 LR: 0.000500\n",
      "Iteration 4100 of 10000. Loss: 0.853148 LR: 0.000500\n",
      "Iteration 4200 of 10000. Loss: 0.851491 LR: 0.000500\n",
      "Iteration 4300 of 10000. Loss: 0.849866 LR: 0.000500\n",
      "Iteration 4400 of 10000. Loss: 0.848274 LR: 0.000500\n",
      "Iteration 4500 of 10000. Loss: 0.846713 LR: 0.000500\n",
      "Iteration 4600 of 10000. Loss: 0.845184 LR: 0.000500\n",
      "Iteration 4700 of 10000. Loss: 0.843686 LR: 0.000500\n",
      "Iteration 4800 of 10000. Loss: 0.842219 LR: 0.000500\n",
      "Iteration 4900 of 10000. Loss: 0.840782 LR: 0.000500\n",
      "Iteration 5000 of 10000. Loss: 0.839375 LR: 0.000500\n",
      "Iteration 5100 of 10000. Loss: 0.837998 LR: 0.000500\n",
      "Iteration 5200 of 10000. Loss: 0.836650 LR: 0.000500\n",
      "Iteration 5300 of 10000. Loss: 0.835332 LR: 0.000500\n",
      "Iteration 5400 of 10000. Loss: 0.834042 LR: 0.000500\n",
      "Iteration 5500 of 10000. Loss: 0.832780 LR: 0.000500\n",
      "Iteration 5600 of 10000. Loss: 0.831546 LR: 0.000500\n",
      "Iteration 5700 of 10000. Loss: 0.830341 LR: 0.000500\n",
      "Iteration 5800 of 10000. Loss: 0.829162 LR: 0.000500\n",
      "Iteration 5900 of 10000. Loss: 0.828011 LR: 0.000500\n",
      "Iteration 6000 of 10000. Loss: 0.826886 LR: 0.000500\n",
      "Iteration 6100 of 10000. Loss: 0.825789 LR: 0.000500\n",
      "Iteration 6200 of 10000. Loss: 0.824717 LR: 0.000500\n",
      "Iteration 6300 of 10000. Loss: 0.823671 LR: 0.000500\n",
      "Iteration 6400 of 10000. Loss: 0.822651 LR: 0.000500\n",
      "Iteration 6500 of 10000. Loss: 0.821656 LR: 0.000500\n",
      "Iteration 6600 of 10000. Loss: 0.820686 LR: 0.000500\n",
      "Iteration 6700 of 10000. Loss: 0.819742 LR: 0.000500\n",
      "Iteration 6800 of 10000. Loss: 0.818821 LR: 0.000500\n",
      "Iteration 6900 of 10000. Loss: 0.817925 LR: 0.000500\n",
      "Iteration 7000 of 10000. Loss: 0.817053 LR: 0.000500\n",
      "Iteration 7100 of 10000. Loss: 0.816204 LR: 0.000500\n",
      "Iteration 7200 of 10000. Loss: 0.815379 LR: 0.000500\n",
      "Iteration 7300 of 10000. Loss: 0.814577 LR: 0.000500\n",
      "Iteration 7400 of 10000. Loss: 0.813798 LR: 0.000500\n",
      "Iteration 7500 of 10000. Loss: 0.813042 LR: 0.000500\n",
      "Iteration 7600 of 10000. Loss: 0.812308 LR: 0.000500\n",
      "Iteration 7700 of 10000. Loss: 0.811596 LR: 0.000500\n",
      "Iteration 7800 of 10000. Loss: 0.810906 LR: 0.000500\n",
      "Iteration 7900 of 10000. Loss: 0.810238 LR: 0.000500\n",
      "Iteration 8000 of 10000. Loss: 0.809591 LR: 0.000500\n",
      "Iteration 8100 of 10000. Loss: 0.808965 LR: 0.000500\n",
      "Iteration 8200 of 10000. Loss: 0.808361 LR: 0.000500\n",
      "Iteration 8300 of 10000. Loss: 0.807777 LR: 0.000500\n",
      "Iteration 8400 of 10000. Loss: 0.807213 LR: 0.000500\n",
      "Iteration 8500 of 10000. Loss: 0.806670 LR: 0.000500\n",
      "Iteration 8600 of 10000. Loss: 0.806146 LR: 0.000500\n",
      "Iteration 8700 of 10000. Loss: 0.805643 LR: 0.000500\n",
      "Iteration 8800 of 10000. Loss: 0.805159 LR: 0.000500\n",
      "Iteration 8900 of 10000. Loss: 0.804694 LR: 0.000500\n",
      "Iteration 9000 of 10000. Loss: 0.804249 LR: 0.000500\n",
      "Iteration 9100 of 10000. Loss: 0.803823 LR: 0.000500\n",
      "Iteration 9200 of 10000. Loss: 0.803415 LR: 0.000500\n",
      "Iteration 9300 of 10000. Loss: 0.803026 LR: 0.000500\n",
      "Iteration 9400 of 10000. Loss: 0.802655 LR: 0.000500\n",
      "Iteration 9500 of 10000. Loss: 0.802302 LR: 0.000500\n",
      "Iteration 9600 of 10000. Loss: 0.801967 LR: 0.000500\n",
      "Iteration 9700 of 10000. Loss: 0.801650 LR: 0.000500\n",
      "Iteration 9800 of 10000. Loss: 0.801351 LR: 0.000500\n",
      "Iteration 9900 of 10000. Loss: 0.801068 LR: 0.000500\n"
     ]
    }
   ],
   "source": [
    "lr = 0.0005               # Try changing the learning rate\n",
    "reg_const = 0.5          # Try changing the regularization constant\n",
    "add_bias = True        # Does adding bias help? Try changing between True and False\n",
    "num_iter = 10000        # Do not change\n",
    "\n",
    "model1 = LogisticRegression(add_bias) \n",
    "model1.train(X_train_bow, Y_train_bow, lr, reg_const)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Set Accuracy using Bag-of-words features\n",
    "\n",
    "We use the function `score` of class `LogisticRegression` in the file `model.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Validation Set Accuracy -  0.9822222222222222\n"
     ]
    }
   ],
   "source": [
    "val_acc = model1.score(X_val_bow, Y_val_bow)\n",
    "print(\"Final Validation Set Accuracy - \", val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters for above model get recorded\n",
    "\n",
    "These hyperparameters will be your submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_hyper('bow', add_bias, lr, reg_const)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Tf-Idf based features (for the whole data)\n",
    "\n",
    "#### To run this block: \n",
    "Complete the `preprocess` and `extract` functions in the `TfIdfFeatureExtractor` class in `feature.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model and preprocess\n",
    "tfidf = TfIdfFeatureExtractor()\n",
    "tfidf.preprocess(business + sports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract fetures and create a numpy array of features\n",
    "X_business_tfidf = tfidf.extract(business)\n",
    "X_sports_tfidf = tfidf.extract(sports)\n",
    "X_data_tfidf = np.concatenate((X_business_tfidf, X_sports_tfidf))\n",
    "\n",
    "# We know the correct labels, so create a numpy array for correct labels\n",
    "# Business -> 0, Sports -> 1\n",
    "Y_data_tfidf = np.concatenate((np.zeros(X_business_tfidf.shape[0]), np.ones(X_sports_tfidf.shape[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting into train and val sets\n",
    "\n",
    "General convention is to have a train/val split in the ratio of 70/30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_train = X_business_tfidf.shape[0] + X_sports_tfidf.shape[0]\n",
    "num_val = np.int(num_train*0.3)\n",
    "X_data_tfidf = np.concatenate((X_business_tfidf, X_sports_tfidf))\n",
    "Y_data_tfidf = np.concatenate((np.zeros(X_business_tfidf.shape[0]), np.ones(X_sports_tfidf.shape[0])))\n",
    "\n",
    "# Data_idxs have been used from Bag of words section, so that we can fairly compare accuracies\n",
    "\n",
    "# Separate train data\n",
    "X_train_tfidf = X_data_tfidf[data_idxs[num_val:], :]\n",
    "Y_train_tfidf = Y_data_tfidf[data_idxs[num_val:]]\n",
    "\n",
    "# Separate val data\n",
    "X_val_tfidf = X_data_tfidf[data_idxs[:num_val], :]\n",
    "Y_val_tfidf = Y_data_tfidf[data_idxs[:num_val]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune hyperparameters here\n",
    "The model will get trained and the loss at various iterations will be printed. Try and reduce this loss through hyperparameter tuning, to get a better validation set accuracy in the next block. However, don't chase the number to a 1.0, as the focus is on implementation rather than winning a contest.\n",
    "\n",
    "You should have already implemented all the necessary functions to run this block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 of 10000. Loss: 1.052076 LR: 0.000500\n",
      "Iteration 100 of 10000. Loss: 0.923512 LR: 0.000500\n",
      "Iteration 200 of 10000. Loss: 0.908375 LR: 0.000500\n",
      "Iteration 300 of 10000. Loss: 0.893891 LR: 0.000500\n",
      "Iteration 400 of 10000. Loss: 0.880035 LR: 0.000500\n",
      "Iteration 500 of 10000. Loss: 0.866779 LR: 0.000500\n",
      "Iteration 600 of 10000. Loss: 0.854097 LR: 0.000500\n",
      "Iteration 700 of 10000. Loss: 0.841964 LR: 0.000500\n",
      "Iteration 800 of 10000. Loss: 0.830357 LR: 0.000500\n",
      "Iteration 900 of 10000. Loss: 0.819251 LR: 0.000500\n",
      "Iteration 1000 of 10000. Loss: 0.808625 LR: 0.000500\n",
      "Iteration 1100 of 10000. Loss: 0.798457 LR: 0.000500\n",
      "Iteration 1200 of 10000. Loss: 0.788727 LR: 0.000500\n",
      "Iteration 1300 of 10000. Loss: 0.779415 LR: 0.000500\n",
      "Iteration 1400 of 10000. Loss: 0.770503 LR: 0.000500\n",
      "Iteration 1500 of 10000. Loss: 0.761972 LR: 0.000500\n",
      "Iteration 1600 of 10000. Loss: 0.753806 LR: 0.000500\n",
      "Iteration 1700 of 10000. Loss: 0.745988 LR: 0.000500\n",
      "Iteration 1800 of 10000. Loss: 0.738503 LR: 0.000500\n",
      "Iteration 1900 of 10000. Loss: 0.731336 LR: 0.000500\n",
      "Iteration 2000 of 10000. Loss: 0.724473 LR: 0.000500\n",
      "Iteration 2100 of 10000. Loss: 0.717900 LR: 0.000500\n",
      "Iteration 2200 of 10000. Loss: 0.711605 LR: 0.000500\n",
      "Iteration 2300 of 10000. Loss: 0.705576 LR: 0.000500\n",
      "Iteration 2400 of 10000. Loss: 0.699801 LR: 0.000500\n",
      "Iteration 2500 of 10000. Loss: 0.694269 LR: 0.000500\n",
      "Iteration 2600 of 10000. Loss: 0.688970 LR: 0.000500\n",
      "Iteration 2700 of 10000. Loss: 0.683893 LR: 0.000500\n",
      "Iteration 2800 of 10000. Loss: 0.679030 LR: 0.000500\n",
      "Iteration 2900 of 10000. Loss: 0.674371 LR: 0.000500\n",
      "Iteration 3000 of 10000. Loss: 0.669908 LR: 0.000500\n",
      "Iteration 3100 of 10000. Loss: 0.665633 LR: 0.000500\n",
      "Iteration 3200 of 10000. Loss: 0.661537 LR: 0.000500\n",
      "Iteration 3300 of 10000. Loss: 0.657614 LR: 0.000500\n",
      "Iteration 3400 of 10000. Loss: 0.653856 LR: 0.000500\n",
      "Iteration 3500 of 10000. Loss: 0.650257 LR: 0.000500\n",
      "Iteration 3600 of 10000. Loss: 0.646811 LR: 0.000500\n",
      "Iteration 3700 of 10000. Loss: 0.643510 LR: 0.000500\n",
      "Iteration 3800 of 10000. Loss: 0.640351 LR: 0.000500\n",
      "Iteration 3900 of 10000. Loss: 0.637326 LR: 0.000500\n",
      "Iteration 4000 of 10000. Loss: 0.634431 LR: 0.000500\n",
      "Iteration 4100 of 10000. Loss: 0.631660 LR: 0.000500\n",
      "Iteration 4200 of 10000. Loss: 0.629010 LR: 0.000500\n",
      "Iteration 4300 of 10000. Loss: 0.626474 LR: 0.000500\n",
      "Iteration 4400 of 10000. Loss: 0.624050 LR: 0.000500\n",
      "Iteration 4500 of 10000. Loss: 0.621732 LR: 0.000500\n",
      "Iteration 4600 of 10000. Loss: 0.619518 LR: 0.000500\n",
      "Iteration 4700 of 10000. Loss: 0.617402 LR: 0.000500\n",
      "Iteration 4800 of 10000. Loss: 0.615381 LR: 0.000500\n",
      "Iteration 4900 of 10000. Loss: 0.613452 LR: 0.000500\n",
      "Iteration 5000 of 10000. Loss: 0.611612 LR: 0.000500\n",
      "Iteration 5100 of 10000. Loss: 0.609857 LR: 0.000500\n",
      "Iteration 5200 of 10000. Loss: 0.608185 LR: 0.000500\n",
      "Iteration 5300 of 10000. Loss: 0.606591 LR: 0.000500\n",
      "Iteration 5400 of 10000. Loss: 0.605075 LR: 0.000500\n",
      "Iteration 5500 of 10000. Loss: 0.603632 LR: 0.000500\n",
      "Iteration 5600 of 10000. Loss: 0.602260 LR: 0.000500\n",
      "Iteration 5700 of 10000. Loss: 0.600957 LR: 0.000500\n",
      "Iteration 5800 of 10000. Loss: 0.599721 LR: 0.000500\n",
      "Iteration 5900 of 10000. Loss: 0.598549 LR: 0.000500\n",
      "Iteration 6000 of 10000. Loss: 0.597439 LR: 0.000500\n",
      "Iteration 6100 of 10000. Loss: 0.596389 LR: 0.000500\n",
      "Iteration 6200 of 10000. Loss: 0.595396 LR: 0.000500\n",
      "Iteration 6300 of 10000. Loss: 0.594460 LR: 0.000500\n",
      "Iteration 6400 of 10000. Loss: 0.593578 LR: 0.000500\n",
      "Iteration 6500 of 10000. Loss: 0.592748 LR: 0.000500\n",
      "Iteration 6600 of 10000. Loss: 0.591969 LR: 0.000500\n",
      "Iteration 6700 of 10000. Loss: 0.591239 LR: 0.000500\n",
      "Iteration 6800 of 10000. Loss: 0.590557 LR: 0.000500\n",
      "Iteration 6900 of 10000. Loss: 0.589920 LR: 0.000500\n",
      "Iteration 7000 of 10000. Loss: 0.589328 LR: 0.000500\n",
      "Iteration 7100 of 10000. Loss: 0.588779 LR: 0.000500\n",
      "Iteration 7200 of 10000. Loss: 0.588272 LR: 0.000500\n",
      "Iteration 7300 of 10000. Loss: 0.587805 LR: 0.000500\n",
      "Iteration 7400 of 10000. Loss: 0.587377 LR: 0.000500\n",
      "Iteration 7500 of 10000. Loss: 0.586987 LR: 0.000500\n",
      "Iteration 7600 of 10000. Loss: 0.586634 LR: 0.000500\n",
      "Iteration 7700 of 10000. Loss: 0.586317 LR: 0.000500\n",
      "Iteration 7800 of 10000. Loss: 0.586034 LR: 0.000500\n",
      "Iteration 7900 of 10000. Loss: 0.585785 LR: 0.000500\n",
      "Iteration 8000 of 10000. Loss: 0.585568 LR: 0.000500\n",
      "Iteration 8100 of 10000. Loss: 0.585383 LR: 0.000500\n",
      "Iteration 8200 of 10000. Loss: 0.585228 LR: 0.000500\n",
      "Iteration 8300 of 10000. Loss: 0.585103 LR: 0.000500\n",
      "Iteration 8400 of 10000. Loss: 0.585007 LR: 0.000500\n",
      "Iteration 8500 of 10000. Loss: 0.584939 LR: 0.000500\n",
      "Iteration 8600 of 10000. Loss: 0.584898 LR: 0.000500\n",
      "Iteration 8700 of 10000. Loss: 0.584884 LR: 0.000500\n",
      "Iteration 8800 of 10000. Loss: 0.584883 LR: 0.000000\n",
      "Iteration 8900 of 10000. Loss: 0.584883 LR: 0.000000\n",
      "Iteration 9000 of 10000. Loss: 0.584883 LR: 0.000000\n",
      "Iteration 9100 of 10000. Loss: 0.584883 LR: 0.000000\n",
      "Iteration 9200 of 10000. Loss: 0.584883 LR: 0.000000\n",
      "Iteration 9300 of 10000. Loss: 0.584883 LR: 0.000000\n",
      "Iteration 9400 of 10000. Loss: 0.584883 LR: 0.000000\n",
      "Iteration 9500 of 10000. Loss: 0.584883 LR: 0.000000\n",
      "Iteration 9600 of 10000. Loss: 0.584883 LR: 0.000000\n",
      "Iteration 9700 of 10000. Loss: 0.584883 LR: 0.000000\n",
      "Iteration 9800 of 10000. Loss: 0.584883 LR: 0.000000\n",
      "Iteration 9900 of 10000. Loss: 0.584883 LR: 0.000000\n"
     ]
    }
   ],
   "source": [
    "lr = 0.0005               # Try changing the learning rate\n",
    "reg_const = 0.5         # Try changing the regularization constant\n",
    "add_bias = True        # Does adding bias help? Try changing between True and False\n",
    "num_iter = 10000        # Do not change\n",
    "\n",
    "model2 = LogisticRegression(add_bias)\n",
    "model2.train(X_train_tfidf, Y_train_tfidf, lr, reg_const)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Set Accuracy using Bag-of-words features\n",
    "\n",
    "We use the function `score` of class `LogisticRegression` in the file `model.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Validation Set Accuracy -  0.9922222222222222\n"
     ]
    }
   ],
   "source": [
    "val_acc = model2.score(X_val_tfidf, Y_val_tfidf)\n",
    "print(\"Final Validation Set Accuracy - \", val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters for above model get recorded\n",
    "\n",
    "These hyperparameters will be your submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_hyper('tf-idf', add_bias, lr, reg_const)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
